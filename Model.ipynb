{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAxU4oXJvbvK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "ZQ1n_YOUm6aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuTbIhf1wkzT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch import optim\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Added myself\n",
        "import torchvision.transforms as tvt\n",
        "import torchaudio.transforms as tat\n",
        "import wandb\n",
        "wandb.init()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaT2BGT-wq6K"
      },
      "outputs": [],
      "source": [
        "class LibriSamples(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_path, partition= \"train\"): \n",
        "\n",
        "        self.X_dir = data_path + \"/\" + \"MFCCs/\"\n",
        "        self.Y_dir = data_path + \"/\" + \"Text Embeddings/\"\n",
        "        self.Z_dir = data_path\n",
        "\n",
        "        self.df_scores = pd.read_csv(self.Z_dir + \"/Mean_Scores.csv\")\n",
        "        self.partition = partition\n",
        "        if self.partition=='train':\n",
        "          self.df_scores=self.df_scores[:512]\n",
        "        if self.partition=='val':\n",
        "          self.df_scores=self.df_scores[512:]\n",
        "        \n",
        "        self.df_scores=self.df_scores.reset_index()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_scores)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        X_path = self.X_dir + self.df_scores['Clip_ID'][ind] + \".npy\"\n",
        "        X = np.load(X_path)\n",
        "        Y_path = self.Y_dir + self.df_scores['Clip_ID'][ind] + \".npy\"\n",
        "        Y = np.load(Y_path)\n",
        "\n",
        "        z_ext = self.df_scores['Extraversion'][ind]\n",
        "        z_agr = self.df_scores['Agreeableness'][ind]\n",
        "        z_con = self.df_scores['Conscientiousness'][ind]\n",
        "        z_neu = self.df_scores['Neuroticism'][ind]\n",
        "        z_ope = self.df_scores['Openness'][ind]\n",
        "\n",
        "\n",
        "        arr=np.zeros(5)\n",
        "        arr[0]=z_ext\n",
        "        arr[1]=z_agr\n",
        "        arr[2]=z_con\n",
        "        arr[3]=z_neu\n",
        "        arr[4]=z_ope\n",
        "        z_vector=torch.from_numpy((arr>0.5))\n",
        "        z_vector=z_vector.type(torch.float)\n",
        "        X = torch.from_numpy(X)\n",
        "        Y = torch.from_numpy(Y)\n",
        "        \n",
        "        return X, Y, z_vector\n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        batch_x  = [torch.nn.functional.pad(x, (0,401-x.shape[2],0,0), value = 0) for x,y,z_vector in batch]\n",
        "        batch_y  = [y for x,y,z_vector in batch]\n",
        "        batch_z_vector = [z_vector for x,y,z_vector in batch]\n",
        "\n",
        "\n",
        "\n",
        "        # return torch.cat(batch_x), torch.stack(batch_y), torch.tensor(batch_z1), torch.tensor(batch_z2), torch.tensor(batch_z3), torch.tensor(batch_z4), torch.tensor(batch_z5)\n",
        "        return torch.cat(batch_x), torch.stack(batch_y), torch.stack(batch_z_vector)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ1kL3iv6uVR"
      },
      "outputs": [],
      "source": [
        "root=\"/content/drive/MyDrive/IDL Project/Final-Submission/Merged Dataset\"\n",
        "train_data = LibriSamples(root, 'train')\n",
        "val_data = LibriSamples(root,'val')\n",
        "train_loader = DataLoader(train_data, batch_size = 8, shuffle = True, num_workers = 0, collate_fn = train_data.collate_fn) \n",
        "val_loader = DataLoader(val_data, batch_size = 8, shuffle = False, num_workers = 0, collate_fn = train_data.collate_fn) \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#While we have desgined the architecture on our own, it is inspired by a homework in another course thaught at CMU, that each member of the team has taken.\n",
        "#This course is 16-824 Visual Learning and Recognition which we have taken in Spring 2022.\n",
        "#The homework was based on the https://arxiv.org/pdf/1606.00061.pdf"
      ],
      "metadata": {
        "id": "uixUEDU5e83j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOJ2d3hTJMyV"
      },
      "outputs": [],
      "source": [
        "class QuestionFeatureExtractor(nn.Module):\n",
        "      \"\"\"\n",
        "      Inputs:\n",
        "          Q: question_encoding in a shape of B x T x word_inp_size\n",
        "      Outputs:\n",
        "          qw: word-level feature in a shape of B x T x embedding_size\n",
        "          qs: phrase-level feature in a shape of B x T x embedding_size\n",
        "          qt: sentence-level feature in a shape of B x T x embedding_size\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "class AlternatingCoAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The Alternating Co-Attention module as in (Lu et al, 2017) paper Sec. 3.3.\n",
        "    \"\"\"\n",
        "    def __init__(self, d=40, k=40, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d = d\n",
        "        self.k = k\n",
        "\n",
        "        self.Wx1 = nn.Linear(d, k)\n",
        "        self.whx1 = nn.Linear(k, 1)\n",
        "\n",
        "        self.Wx2 = nn.Linear(d, k)\n",
        "        self.Wg2 = nn.Linear(d, k)\n",
        "        self.whx2 = nn.Linear(k, 1)\n",
        "\n",
        "        self.Wx3 = nn.Linear(d, k)\n",
        "        self.Wg3 = nn.Linear(d, k)\n",
        "        self.whx3 = nn.Linear(k, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, Q, V):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            Q: question feature in a shape of BxTxd\n",
        "            V: image feature in a shape of BxNxd\n",
        "        Outputs:\n",
        "            shat: attended question feature in a shape of Bxk\n",
        "            vhat: attended image feature in a shape of Bxk\n",
        "        \"\"\"\n",
        "        B = Q.shape[0]\n",
        "\n",
        "        # 1st step\n",
        "        H = torch.tanh(self.Wx1(Q))\n",
        "        H = self.dropout(H)\n",
        "        ax = F.softmax(self.whx1(H), dim=1)\n",
        "        shat = torch.sum(Q * ax, dim=1, keepdim=True)\n",
        "\n",
        "        # 2nd step\n",
        "        H = torch.tanh(self.Wx2(V) + self.Wg2(shat))\n",
        "        H = self.dropout(H)\n",
        "        ax = F.softmax(self.whx2(H), dim=1)\n",
        "        vhat = torch.sum(V * ax, dim=1, keepdim=True)\n",
        "\n",
        "        # 3rd step\n",
        "        H = torch.tanh(self.Wx3(Q) + self.Wg3(vhat))\n",
        "        H = self.dropout(H)\n",
        "        ax = F.softmax(self.whx3(H), dim=1)\n",
        "        shat2 = torch.sum(Q * ax, dim=1, keepdim=True)\n",
        "\n",
        "        return shat2.squeeze(), vhat.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3ssCQHzVljn"
      },
      "outputs": [],
      "source": [
        "class CoattentionNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Predicts an answer to a question about an image using the Hierarchical Question-Image Co-Attention\n",
        "    for Visual Question Answering (Lu et al, 2017) paper.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.text_feats_layer = nn.Linear(1024, 40)\n",
        "\n",
        "        self.attention_layer = AlternatingCoAttention()\n",
        "\n",
        "        self.Ww = nn.Linear(40, 40)\n",
        "        self.Ws = nn.Linear(40, 30)\n",
        "\n",
        "        self.dropout = nn.Dropout(p = 0.1) \n",
        "\n",
        "        self.classifier = nn.Linear(30, 5)\n",
        "\n",
        "    def forward(self, audio_feats, text_feats):\n",
        "        text_feats = text_feats.unsqueeze(dim = 1)\n",
        "        text_feats = self.text_feats_layer(text_feats)\n",
        "        \n",
        "        audio_feats = audio_feats.permute(0, 2, 1)\n",
        "        shat, vhat = self.attention_layer(text_feats, audio_feats)\n",
        "        hw = torch.tanh(self.Ww(torch.add(shat, vhat)))\n",
        "        output = self.classifier(self.Ws(hw))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72EQThxvdYdh"
      },
      "outputs": [],
      "source": [
        "model = CoattentionNet().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 40\n",
        "# optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr = 4e-4, weight_decay = 1e-8)\n",
        "#loss_fn = nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean', label_smoothing=0.1)\n",
        "loss_fn = nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * num_epochs, eta_min=0, last_epoch=- 1, verbose=False)"
      ],
      "metadata": {
        "id": "uTwO3fMLGAf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as sm\n",
        "def validate(val_loader,model):\n",
        "    num_correct=0\n",
        "    outputs=np.array([])\n",
        "    preds=np.array([])\n",
        "    for batch_id, batch_data in enumerate(val_loader):\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "      model.eval() \n",
        "\n",
        "      x, y, z_vector = batch_data\n",
        "      with torch.no_grad():\n",
        "        output = model.forward(x.cuda(), y.cuda())\n",
        "        output = torch.sigmoid(output)\n",
        "        output = (output>0.5).type(torch.int)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        num_correct += int((output.cuda() == z_vector.cuda()).sum())\n",
        "      output_flat=output.cpu().detach().numpy().flatten()\n",
        "      z_vector_flat=z_vector.cpu().detach().numpy().flatten()\n",
        "      outputs=np.append(outputs,output_flat)\n",
        "      preds=np.append(preds,z_vector_flat)\n",
        "        #loss = loss_fn(output, z_vector.cuda())\n",
        "      \n",
        "    \n",
        "    recall=sm.recall_score(preds,outputs)\n",
        "    precision=sm.precision_score(preds,outputs)\n",
        "    f1=sm.f1_score(preds,outputs)\n",
        "    return (num_correct/(128*5),recall,precision,f1)"
      ],
      "metadata": {
        "id": "6mTZl8-8ok3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch Number',epoch+1)\n",
        "        num_batches = len(train_loader)\n",
        "        for batch_id, batch_data in enumerate(train_loader):\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            model.train()\n",
        "            current_step = epoch * num_batches + batch_id\n",
        "\n",
        "            x, y, z_vector = batch_data\n",
        "            output = model.forward(x.cuda(), y.cuda())\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(output, z_vector.cuda())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "        acc,recall,precision,f1=validate(val_loader,model)\n",
        "        print('Val Accuracy: ',acc)\n",
        "        print('Val Recall: ',recall)\n",
        "        print('Val Precision: ',precision)\n",
        "        print(\"Val F1-Score: \",f1)\n",
        "        wandb.log({'epoch':epoch, 'loss': loss, 'val_acc':acc,'recall':recall,'precision':precision,'f1-score':f1})"
      ],
      "metadata": {
        "id": "gHfIZBtUFvrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "UZsN9mHPJKGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yGefffHd4Rj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Model.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}